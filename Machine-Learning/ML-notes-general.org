* GAN
** what are GANs?
*** an answer by [[https://www.quora.com/What-are-Generative-Adversarial-Networks][Ofir Nachum]] from Google Brain
   
Generative Adversarial Networks (GANs) are neural networks that are trained in
an adversarial manner to generate data mimicking some distribution.

To understand this deeply, first you'll have to understand what a generative
model is. In machine learning, the two main classes of models are generative and
discriminative. A discriminative model is one that discriminates between two (or
more) different classes of data - for example a convolutional neural network
that is trained to output 1 given an image of a human face and 0 otherwise. A
generative model on the other hand doesn't know anything about classes of data.
Instead, its purpose is to generate new data which fits the distribution of the
training data - for example, a Gaussian Mixture Model is a generative model
which, after trained on a set of points, is able to generate new random points
which more-or-less fit the distribution of the training data (assuming a GMM is
able to mimic the data well). More specifically, a generative model gg trained
on training data XX sampled from some true distribution DD is one which, given
some standard random distribution ZZ, produces a distribution D′D′ which is
close to DD according to some closeness metric (a sample z∼Zz∼Z maps to a sample
g(z)∼D′g(z)∼D′).

The 'standard' way to determine a generative model gg given training data XX is
maximum-likelihood, which requires all sorts of calculations of marginal
probabilities, partition functions, most-likely estimates, etc. This may be
feasible when your generative model is a GMM, but if you want to try to make a
generative model out of a deep neural network, this quickly becomes intractable.

Adversarial training allows you to train a generative model without all of these
intractable calculations. Let's assume our training data X⊂ℝdX⊂Rd. The basic
idea is that you will have two adversarial models - a generator g:ℝn→ℝdg:Rn→Rd
and a discriminator d:ℝd→{0,1}d:Rd→{0,1}. The generator will be tasked with
taking in a given sample from a standard random distribution (e.g. a sample from
an nn-dimensional Guassian) and producing a point that looks sort of like it
could come from the same distribution as XX. The discriminator, on the other
hand, will be tasked with discriminating between samples from the true data XX
and the artificial data generated by gg. Each model is trying to best the
other - the generator's objective is to fool the discriminator and the
discriminator's objective is to not be fooled by the generator.

In our case, both gg and dd are neural nets. And what happens is that we train
them both in an alternating manner. Each of their objectives can be expressed as
a loss function that we can optimize via gradient descent. So we train gg for a
couple steps, then train dd for a couple steps, then give gg the chance to
improve itself, and so on. The result is that the generator and the
discriminator each get better at their objectives in tandem, so that at the end,
the generator is able to or is close to being able to fool the most
sophisticated discriminator. In practice, this method ends up with generative
neural nets that are incredibly good at producing new data (e.g. random pictures
of human faces).
*** an brief summary from zhihu
G和D分别为三层的全链接的神经网络，其中G的激活函数分别为，relu，sigmoid，liner，这里前两层只是因为考虑到数据的非线性转换，并没有什么特别选择这两个激活函数的原因。其次D的三层分别为relu，sigmoid，sigmoid。

接下来就引出GAN的训练问题。GAN的思想源于博弈论，直白一点就是套路与反套路。

先从D开始分析，D作为辨别器，它的职责就是区分于真实的高斯分布和G生成的”假”高斯分布。所以很显然，针对D来说，其需要解决的就是传统的二分类问题。

在二分类问题中，我们习惯用交叉熵来衡量分类效果。从公式中不难看出，在全部分类正确时，交叉熵会接近于0，因此，我们的目标就是通过拟合D的参数来最小化交叉熵的值。

D既然是传统的二分类问题，那么D的训练过程也很容易得出了

+ 即先把真实数据标识为‘1’(真实分布)，由生成器生成的数据标识为’0‘(生成分布)，反复迭代训练D ------------ (1)

说G的训练之前先来打个比方，假如一男一女在一起了，现在两人性格出现矛盾了，女生并不愿意改变，但两个人都想继续在一起，这时，唯一的方法就是男生改变了。先忽略现实生活的问题，但从举例的角度说，显然久而久之男生就会变得更加fit这个女生。

G的训练也是如此：

+ 先将G拼接在D的上方，即G的输出作为D的输入（男生女生在一起），而同时固定D的参数（女生不愿意改变），并将进入G的噪音样本标签全部改成'1'（目标是两个人继续在一起，没有其他选择），为了最小化损失函数，此时就只能改变G的每一层权重，反复迭代后G的生成能力因此得以改进。（男生更适合女生） ------------ (2)

反复迭代（1）（2），最终G就会得到较好的生成能力。

* Questions
** 深度学习的限制
著名深度学习库 Keras 作者 François Chollet 曾在一篇名为《深度学习的限制》的文章
中说到：「深度学习唯一真正能成功做到的是使用几何变换，在给定大量人类标注数据的情
况下将空间 X 映射到空间 Y 的能力。」这些空间拥有多维，不仅仅是三维的，这就是深度
学习可以模仿毕加索风格作画、在德州扑克中 Bluff，以及在其他一些方面里展示创造力的
原因。但是对于外行人来说，这也许意味着：深度学习模型可以被训练成拥有识别猫的能力，
但本身不知道什么是猫；可以是一个种族主义者，但不知道什么是种族主义。深度学习可以
识别猫、具有种族主义，并赢得很多游戏，这看起来是令人瞩目的进步，但深度学习无法解
释为什么图中的动物是猫，也无法定义种族主义。
** TODO 机器学习包含哪些学习思想？
https://www.zhihu.com/question/267135168

"我在学习的过程中发现有很多绝妙的点子，不知道有没有同学在总结类似的东西．例如竞
争的思想(Generative Adversarial Network)，贝叶斯后验概率的思想，最大似然的思想，
动量的思想(adam optimization)，投票的思想(random forest, bagging)，对错误值加大
注意力的思想(boosting, Adagrad optimization)，学习自己的思想(Deep Baltzman
Machine, autoencoder)，one-shot learning (siames-neural network)里成对学习的思想，
记忆的思想(Long Short Term Memory)，筛子的思想(CNN)，规模的思想
(scaling,normalization)，自然选择的思想(genetic algorithm)，人工选择的思想
(reinforcement learning),逼近的思想(Markov, hopefield)，未观测信息的影响(hidden
layers)，比例的思想(logorithm)."
* Resources
** Google Machine Learning Crash Course
 https://developers.google.com/machine-learning/crash-course/
