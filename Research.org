* Current Tasks

** TODO Read the LPD & ROAD papers(do the necessary calculations)and figure out a strategy to establish our result

*** Notion of sparsity, how to measure? When will it preserve?

*** DONE Read Fan's main theorem proof
    CLOSED: [2015-03-28 Sat 14:10] SCHEDULED: <2015-03-21 Sat>
*** Exponential inequalities
Need to figure out how the inequalites in lemma 1 were derived in
LPD paper.

**** Berstein Inequality(2 types of conditions)

*** Obtain similar results like (26) and (27) in LPD paper

*** DONE Uniqueness of the LPD estimator
CLOSED: [2015-11-15 Sun 18:43]
*** TODO prove LPD type asymptotics results for correlation matrices
    SCHEDULED: <2015-12-04 Fri>
** comparison of our algorithm with related algorithms like LPD & ROAD
*** what is the pros & cons?

*** TODO Can LPD select the best marginal feature? How about ROAD? [1/3]
when the tuning parameter is near lambda_max, L0 norm = 1 implies the best
marginal feature is in the active set
**** DONE study whether whenever L0 norm = 1, the nonzero feature is the best marginal feature
     CLOSED: [2015-09-09 Wed 16:33] SCHEDULED: <2015-08-28 Fri>
The answer is no, counter example exists.
**** TODO try to find counter example for covariance matrix via simulation construction

**** TODO write down the results you've got so far 
SCHEDULED: <2015-12-04 Fri>

*** piecewise lineararity of the LPD problem & uniqueness

**** DONE professor Lee Dicker's Danzig Selector uniqueness reference
CLOSED: [2015-11-18 Wed 16:13]
** implementation of our algorithm

*** DONE nonsingular case
    CLOSED: [2015-08-15 Sat 14:06]

*** TODO singular case
    SCHEDULED: <2015-12-04 Fri>
    
** regularly write & organize the notes into latex for thesis writing
** TODO Analyze leukemia data
*** Original dataset vs golub dataset in mulltest package?
No conflict, since I found the script which the autors of mulltest used to
preprocess the data into the *golub* dataset in their package.
*** current issues
**** Sig is not p.d., how to get an initial solution
***** DONE idea
      CLOSED: [2015-09-18 Fri 10:41] SCHEDULED: <2015-05-15 Fri>
Use the objective function in section 4 of ROAD paper, write it in
regression form then apply lars pacakge to solve an initial solution
for a lambda>0.
**** modify the algorithm for the case with singular Sig matrix
The current update method relies on the invertibility of the active
set covariance matrix.
***** TODO Q: when will the solution be unique when Sig is singular?
Not easy at current stage
      SCHEDULED: <2015-05-16 Sat>
***** DONE When Sig is singular, starting from an initial solution, how to update the optimal solutions and subgradients?
      CLOSED: [2015-04-16 Thu 16:26] SCHEDULED: <2015-04-08 Wed>
For gamma1 vector, it is easy. But for gamma2 vector, how to choose
it?
***** Any matrix decomposition package available in R, matlab?
**** p>3000, computation is slow in R
*** Weighted case vs Equal weight case
**** idea
Read the code of ROAD and see whether we could modify it to use in the
weighted scenario.
*** DONE Cross Validation
    CLOSED: [2015-08-18 Tue 15:34] SCHEDULED: <2015-08-16 Sun>
How to do CV for the current problem?
** TODO Find other implementation code of CLASSO to compare
*** Matlab version for ROAD
*** Tony Cai's LPD
**** DONE Find/write code to solve the LP problem in the paper
     CLOSED: [2015-07-21 Tue 11:40] SCHEDULED: <2015-05-14 Thu>
** estimation of conditional heteroscedastic time series
* Current needed background/technique/skill
** convex optimization(like l0,l1,l2...), KKT condition
*** Langrange Multiplier Theory
to the extent that I can derive the all the dual problems in the
recent papers I read(ROAD,Cai,LARS,etc) swiftly

know how to derive dual form and solve langrange multiplier equations
**** Duality and Application 
*** gradient descent & stochastic gradient descent(SGD)
** matrix norms and calculus
** inequalities
*** elementary inequality
*** norm inequality
*** concentration inequality
** asymptotics
*** familiarize with the rate of convergence language
** bayesian
*** BDA book
focus on the examples and the computation chapters
* Previous work
** Classo Project Special Case

*** DONE Algorithm
    CLOSED: [2015-02-11 Wed 18:42]

*** DONE Matrix Update
    CLOSED: [2015-02-11 Wed 18:42]


*** Algorithm Check
**** Whether the current version is correct
like stopping rule
**** DONE LARS package implementation
   CLOSED: [2015-02-20 Fri 15:14]
using the lars package, for p=4, the number of pieces doesn't meet the expected 42
* Temporary aside
** TODO Gradually export the texmacs version of the CLASSO notes to a latex version
** TODO Think about how to apply our algorithm in classification
** TODO Think about how to modify the algorithm for extension
* Long Term Improvement
** habit of solving problems 
** habit of make abstraction, combination & generalization
* Thoughts on Thesis

** Take initiative & Communicate Efficiently
*** if stuck when trying to prove sth, try find help
Consider people like Boss Xiao, Shetou, Chunhong, Feng Long, Li Qian
Also consider the internet community
*** find more chances to talk to Boss Xiao
Try to do twice a week, like on Wednesday afternoon
** Practise *mental calculation*
** Work hard & consistently
** Persistently improve on the related math skills
I definitely could improve my Matrix Calculus & Matrix Analysis Skills to a much higher level!!!
** Aha & Crystal Clear Moments!
** Two Trinities: "Body, Skill, Heart", "Math, Stat, Programming"
** What results have you got?
*** written down formally?
**** the ROAD exact algorithm for nonsingular case
**** a result of best marginal feature 
*** scratch or in mind
**** counter example for best marginal feature
**** algorithm for singular covariance matrix
** What results are you currently aiming to obtain?
*** easy ones
**** uniqueness of the LPD 
*** hard ones
**** LPD asymptotics results for correlation matrices 
** Any idea about extension/generalization?
** Idea about data analysis?
** Have the results necessary for a paper? How to organize them?

* Research Habits, Idea & Challenge Plan
** Habits
*** save time & squeeze time
**** ban wechat moments, news checking, etc
**** avoid unnecessary meet and appointment
**** prepare good breakfast, eat quick lunck
*** time analysis
Mon - Wed: 5 hours at night
Thur - Friday: 12 hours per day
Saturday: 9 hours
Sunday: 10 hours

Total: 46 hours/week
Deduction: Sep-Oct, 6 hours lecture preparing per week
*** improve related problem solving skill
as often as possible, better be everyday
****  math/stat problem solving
      SCHEDULED: <2015-09-16 Wed 22:30-23:30 +2d>
**** programming problem solving
     SCHEDULED: <2015-09-17 22:30-23:30 +2d>
*** express your learning and finding in timely manner
*** back up your findings(notes and script) in a timely manner
**** using github
currently I'm maintaining backup repositories for my org, lyx and research r
scripts on github.
*** find projects to challenge yourself in timely manner

** Challenge Plan
*** 9.14 - 9.24
**** DONE primal dual problems examples
     CLOSED: [2015-09-30 Wed 15:11]
**** how can you generalize the sign pattern method for proving piecewise linearity
**** the homotopy idea
*** 11.18 - 12.1 [0/3]
**** TODO write down the uniqueness proof for LPD 
**** TODO Study Lagrangian Duality Theory and accumlate more examples
**** TODO read the concentration inequality section of Tao's random matrix book
*** in the future
**** LPD asymptotics for correlation matrices
**** Improve simulation technique
***** TODO R problems, hard section
** Communication with advisor
*** TODO discuss thesis and graduation with advisor
    SCHEDULED: <2015-11-01 Sun>
** Tao of learning
If you really wanna learn something, always find/generate the *motivation*
first! Then spending enough time/efforts/good communications with others(if possible)
should follow.
